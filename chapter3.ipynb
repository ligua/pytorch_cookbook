{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor,张量\n",
    "\n",
    "粗暴认为它是一个数组，支持高效的科学计算，和numpy中的ndarrays类似\n",
    "\n",
    "从接口API的角度，对tensor的操作分为两类\n",
    "- torch.function : torch.save \n",
    "- tensor.funciton: tensor.view\n",
    "\n",
    "从存储的角度，对tensor的操作分为两类\n",
    "- donnot change itself : a.add(b)\n",
    "- change itself, inplace : a.add_(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate one Tensor\n",
    "- Tensor(*size) \n",
    "- ones\n",
    "- zeros\n",
    "- eye\n",
    "- arange(s, e, step)\n",
    "- rand / randn : random or 标准分布\n",
    "- normal(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        nan  4.5916e-41  0.0000e+00\n",
       " 0.0000e+00  1.8401e+37  0.0000e+00\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  2  3\n",
       " 4  5  6\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist() # change Tensor to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel() # b number of elements in b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " 1.00000e-45 *\n",
       "   1.4013  0.0000  1.4013\n",
       "   0.0000  1.4013  0.0000\n",
       " [torch.FloatTensor of size 2x3], \n",
       "  2\n",
       "  3\n",
       " [torch.FloatTensor of size 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.Tensor(b.size()) # c with the same size as b\n",
    "d = torch.Tensor((2,3))\n",
    "c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape # same as b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0  0\n",
       " 0  0  0\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 3\n",
       " 5\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  1.0000\n",
       "  5.5000\n",
       " 10.0000\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0035  0.9841 -0.4429\n",
       " 0.6638 -0.4723 -0.2171\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2333  0.3416  0.4226\n",
       " 0.6209  0.7170  0.6326\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 4\n",
       " 0\n",
       " 2\n",
       " 1\n",
       " 3\n",
       "[torch.LongTensor of size 5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(5) # random sorting for 5 element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  0  0\n",
       " 0  1  0\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  0  0\n",
       " 0  1  0\n",
       " 0  0  1\n",
       " 0  0  0\n",
       "[torch.FloatTensor of size 4x3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用的Tensor操作\n",
    "\n",
    "- tensor.view : 改变tensor的形状，返回的新tensor和原来的tensor共享内存， 并不会修改自身的数据\n",
    "- squeeze or unsqueeze : 添加维度或者减少维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1  2\n",
       " 3  4  5\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 6)\n",
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1\n",
       " 2  3\n",
       " 4  5\n",
       "[torch.FloatTensor of size 3x2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1, 2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0  1\n",
       "\n",
       "(1 ,.,.) = \n",
       "  2  3\n",
       "\n",
       "(2 ,.,.) = \n",
       "  4  5\n",
       "[torch.FloatTensor of size 3x1x2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0  1\n",
       "\n",
       "(1 ,.,.) = \n",
       "  2  3\n",
       "\n",
       "(2 ,.,.) = \n",
       "  4  5\n",
       "[torch.FloatTensor of size 3x1x2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  0  1  2\n",
       "  3  4  5\n",
       "[torch.FloatTensor of size 1x1x2x3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1,1,1,2,3)\n",
    "c.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1  2\n",
       " 3  4  5\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.squeeze() # squeeze all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0  100\n",
       "   2    3\n",
       "   4    5\n",
       "[torch.FloatTensor of size 3x2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b\n",
    "# share memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0  100    2\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0.0000  100.0000    2.0000\n",
       "   3.0000    4.0000    5.0000\n",
       "   0.0000    0.0000    0.0000\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3,3) # generate new memory space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "索引支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7903  0.0037 -1.6036  0.6245\n",
       "-1.8013  0.2796 -1.0926  0.1169\n",
       " 0.6609  0.5842  0.9660  0.5513\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7903\n",
       " 0.0037\n",
       "-1.6036\n",
       " 0.6245\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7903\n",
       "-1.8013\n",
       " 0.6609\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6035573482513428"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6244868040084839"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7903  0.0037 -1.6036  0.6245\n",
       "-1.8013  0.2796 -1.0926  0.1169\n",
       "[torch.FloatTensor of size 2x4]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0  0  0\n",
       " 0  0  0  0\n",
       " 0  0  0  0\n",
       "[torch.ByteTensor of size 3x4]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  0  0  1\n",
       " 0  1  0  0\n",
       " 1  1  1  1\n",
       "[torch.ByteTensor of size 3x4]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7903\n",
       " 0.6245\n",
       " 0.2796\n",
       " 0.6609\n",
       " 0.5842\n",
       " 0.9660\n",
       " 0.5513\n",
       "[torch.FloatTensor of size 7]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7903  0.0037 -1.6036  0.6245\n",
       "-1.8013  0.2796 -1.0926  0.1169\n",
       "[torch.FloatTensor of size 2x4]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[torch.LongTensor([0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(0, 16).view(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1   2   3\n",
       "  4   5   6   7\n",
       "  8   9  10  11\n",
       " 12  13  14  15\n",
       "[torch.FloatTensor of size 4x4]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1   2   3\n",
       "  4   5   6   7\n",
       "  8   9  10  11\n",
       " 12  13  14  15\n",
       "[torch.FloatTensor of size 4x4]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = torch.LongTensor([[0,1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   5  10  15\n",
       "[torch.FloatTensor of size 1x4]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线的元素\n",
    "index = torch.LongTensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  3\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取反对角线上的元素\n",
    "index = torch.LongTensor([[3,2,1,0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 12   9   6   3\n",
       "[torch.FloatTensor of size 1x4]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取反对角线上的元素\n",
    "index = torch.LongTensor([[3,2,1,0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   5  10  15\n",
       " 12   9   6   3\n",
       "[torch.FloatTensor of size 2x4]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = torch.LongTensor([[0,1,2,3],[3,2,1,0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   3\n",
       "  5   6\n",
       " 10   9\n",
       " 15  12\n",
       "[torch.FloatTensor of size 4x2]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = torch.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [4 x 2] to be smaller size than src [3 x 3] and to be smaller than tensor [4 x 4] apart from dimension 1 at c:\\anaconda2\\conda-bld\\pytorch_1519496000060\\work\\torch\\lib\\th\\generic/THTensorMath.c:593",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-a6b743cb4525>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected index [4 x 2] to be smaller size than src [3 x 3] and to be smaller than tensor [4 x 4] apart from dimension 1 at c:\\anaconda2\\conda-bld\\pytorch_1519496000060\\work\\torch\\lib\\th\\generic/THTensorMath.c:593"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "c = torch.zeros(4,4)\n",
    "c.scatter_(1, index, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy风格的高级索引机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(0, 27).view(3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.0, 24.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,1,2], x[2,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 14\n",
       " 24\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,2],[1,2],[2,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 19\n",
       " 10\n",
       "  1\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2,1,0], [0], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor的类型\n",
    "- torch.FloatTensor\n",
    "- torch.DoubleTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-45 *\n",
       "  0.0000  0.0000  4.2039\n",
       "  0.0000  1.4013  0.0000\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0  0\n",
       " 0  0  0\n",
       "[torch.IntTensor of size 2x3]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逐元素操作\n",
    "输出和输入的形状相同\n",
    "- abs/sqrt/div/exp/fmod/log\n",
    "- cos/sin/asin/atan2/cosh\n",
    "- ceil/round/floor/cosh\n",
    "- clamp\n",
    "- sigmoid/tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000  0.5403 -0.4161\n",
       "-0.9900 -0.6536  0.2837\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 6).view(2,3)\n",
    "torch.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1  2\n",
       " 3  4  5\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1  2\n",
       " 0  1  2\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1  2\n",
       " 0  1  2\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fmod(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1   4\n",
       "  9  16  25\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1   4\n",
       "  9  16  25\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(a, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归并操作\n",
    "- mean/sum/median/mode\n",
    "- norm/dist\n",
    "- std/var\n",
    "- cumsum/cumprod 累加/累乘\n",
    "\n",
    "使得输出形状小于输入形状，并按照某一个维度进行指定的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于dim参数，加入输入的形状为(m, n, k)\n",
    "- dim is 0, output size is (1,n,k) or (n,k)\n",
    "- dim is 1, output size is (m,1,k) or (m,k)\n",
    "- dim is 2, output size is (m,n,k) or (m,n)\n",
    "\n",
    "关于keepdim，为True则保留1的维度，默认不保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2  2  2\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3\n",
       " 3\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1  2\n",
       " 3  4  5\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 6).view(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1   3\n",
       "  3   7  12\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cumsum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1   4\n",
       "  9  16  25\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较操作\n",
    "- gt/lt/ge/le/eq/ne\n",
    "- topk\n",
    "- sort\n",
    "- max/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   3   6\n",
       "  9  12  15\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.linspace(0, 15, 6).view(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 15  12   9\n",
       "  6   3   0\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.linspace(15, 0, 6).view(2,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  15\n",
       "   6\n",
       " [torch.FloatTensor of size 2], \n",
       "  0\n",
       "  0\n",
       " [torch.LongTensor of size 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(b, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个返回值，第一行和第二行的最大值，最大值分别是这一行的第几个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 10  10  10\n",
       " 10  12  15\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(a, min=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数\n",
    "\n",
    "|name|func|\n",
    "| --------   | -----:   | :----: |\n",
    "|trace|对角线元素之和|\n",
    "\n",
    "|diag\n",
    "- triu\n",
    "- mm/bmm\n",
    "- addmm/addbmm/addmv\n",
    "- t\n",
    "- dot/cross\n",
    "- inverse\n",
    "- svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor和numpy之间进行转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.DoubleTensor of size 2x3]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   1  100    1\n",
       "   1    1    1\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0, 1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   1  200    1\n",
       "   1    1    1\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b and c share memory\n",
    "c[0, 1] = 200\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor的内部结构\n",
    "分为信息区Tensor和存储区Storage\n",
    "- Tensor:size, stride, type\n",
    "- Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0, 6)\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the id of the object is the memory address of this object\n",
    "id(b.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0  100    2\n",
       "   3    4    5\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 100.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:]\n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086690127048"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086690127040"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_ptr()会返回tensor首元素的内存地址，相差8是因为2个float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0\n",
       " 100\n",
       "-100\n",
       "   3\n",
       "   4\n",
       "   5\n",
       "[torch.FloatTensor of size 6]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-100\n",
       "   3\n",
       "   4\n",
       "   5\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.Tensor(c.storage())\n",
    "d[0] = 6666\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 6666   100  -100\n",
       "    3     4     5\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到下面的几个tensor都是共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 2)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), b.storage_offset(), c.storage_offset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绝大多操作并不修改storage信息，而是改变tensor的头部信息，节省内存，并且提升了处理速度。\n",
    "\n",
    "一般高级索引都不和原来的tensor共享内存，但是普通索引则可以。\n",
    "\n",
    "即numpy风格的高级索引。\n",
    "\n",
    "因为普通索引可以通过修改tensor头部信息（offset，stride，size）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor持久化\n",
    "\n",
    "torch.save\n",
    "\n",
    "torch.load\n",
    "\n",
    "load时可以将GPU tenosr指定映射到CPU或者GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(a, 'a.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a, b is GPU 0\n",
    "b = torch.load('a.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a, c is CPU\n",
    "c = torch.load('a.pth', map_location=lambda storage, loc : storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 6666\n",
       "  100\n",
       " -100\n",
       "    3\n",
       "    4\n",
       "    5\n",
       "[torch.cuda.FloatTensor of size 6 (GPU 0)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor的向量化\n",
    "\n",
    "向量化计算是一种特殊的并行化计算方式。\n",
    "\n",
    "一般程序在同一个时间只会执行一个操作，\n",
    "\n",
    "向量化计算能够在同一时间执行多个操作，通常是对不同的数据执行同样的一个或者一批指令，即将指令应用在一个数组/向量上。\n",
    "\n",
    "常见即是避免使用for循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i, j in zip(x, y):\n",
    "        result.append(i+j)\n",
    "    return torch.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 µs ± 20.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "The slowest run took 15.32 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "8.12 µs ± 9.26 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(100)\n",
    "y = torch.zeros(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到有超过10倍的差异，实际使用中要尽量使用内建函数（builtin-function），函数由底层的C/C++实现，经由底层优化实现高效计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 torch.set_num_threads设置多线程并行计算时所占用的线程数，从而限制所占用的CPU数目。\n",
    "\n",
    "使用 torch.set_printoptions来设置打印tensor时的数值精度和格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ce94c88ba8>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cebd6d5828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# set random seed\n",
    "# to confirm same result in different computers\n",
    "\n",
    "torch.manual_seed(1000)\n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' random data y = x * 2 + 3 '''\n",
    "    x = torch.rand(batch_size, 1)*20\n",
    "    y = x * 2 + (1 + torch.randn(batch_size, 1))*3\n",
    "    return x, y\n",
    "\n",
    "x, y = get_fake_data()\n",
    "\n",
    "plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init parameters\n",
    "w = torch.rand(1, 1)\n",
    "b = torch.zeros(1,1)\n",
    "lr = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ce94d81dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0116281509399414 3.018310308456421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ii in range(20000):\n",
    "    x, y = get_fake_data()\n",
    "\n",
    "    # forward\n",
    "    y_pred = x.mm(w) + b.expand_as(y)\n",
    "    loss = 0.5 * (y_pred - y) ** 2\n",
    "    loss = loss.sum()\n",
    "\n",
    "    # backward\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "\n",
    "    # update parameters\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "\n",
    "    if ii % 1000 == 0:\n",
    "        # draw\n",
    "        display.clear_output(wait=True)\n",
    "        x = torch.arange(0, 20).view(-1, 1)\n",
    "        y = x.mm(w) + b.expand_as(x)\n",
    "        plt.plot(x.numpy(), y.numpy()) # predicted\n",
    "\n",
    "        x2, y2 = get_fake_data(batch_size=20)\n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "\n",
    "        plt.xlim(0, 20)\n",
    "        plt.ylim(0, 41)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "        print(w.squeeze()[0], b.squeeze()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图是现代深度学习框架的核心，为自动求导算法--反向传播（Back Propogation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable\n",
    "Variable封装了tensor，并记录对tensor的操作记录来构建计算图，主要包含三个属性\n",
    "- data: variable所包含的tensor\n",
    "- grad: 保存data对应的梯度，grad也是variable，和data的形状一致\n",
    "- grad_fn: 指向一个Function，记录tensor的操作历史，即是什么操作的输出\n",
    "\n",
    "构造函数需要传入tensor，同时有两个参数可以选择。\n",
    "- requires_grad(bool):是否要对该Variable进行求导\n",
    "- volatile（bool):挥发，构建在这个variable上面的图都不会求导，专为推理阶段设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算各个Variable的梯度，则只需要调用根节点variable的backward方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。\n",
    "\n",
    "variable.backward(grad_variables=None, retain_graph=None, create_graph=None)\n",
    "- grad_variable:\n",
    "- retain_graph:\n",
    "- create_graph:对反向传播过程再次构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1  1  1\n",
       " 1  1  1  1\n",
       " 1  1  1  1\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Variable(torch.ones(3,4), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0  0  0  0\n",
       " 0  0  0  0\n",
       " 0  0  0  0\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Variable(torch.zeros(3, 4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1  1  1\n",
       " 1  1  1  1\n",
       " 1  1  1  1\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a+b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 12\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = c.sum()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1  1  1\n",
       " 1  1  1  1\n",
       " 1  1  1  1\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 是否为叶子结点\n",
    "a.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    '''compute y'''\n",
    "    y = x ** 2 * torch.exp(x)\n",
    "    return y\n",
    "\n",
    "def gardf(x):\n",
    "    '''by hand'''\n",
    "    dx = 2*x*torch.exp(x) + x**2*torch.exp(x)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5354  0.1078  8.1789  0.4551\n",
       " 1.0608  0.0260  2.5683  0.3988\n",
       " 0.4398  2.8664  0.1772  0.2013\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.randn(3,4), requires_grad=True)\n",
    "y = f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -0.0604  -0.4295  19.7654  -0.2569\n",
       "  4.0117  -0.2694   7.8032  -0.3343\n",
       " -0.2805   8.4989  -0.4604  -0.4608\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(torch.ones(y.size()))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -0.0604  -0.4295  19.7654  -0.2569\n",
       "  4.0117  -0.2694   7.8032  -0.3343\n",
       " -0.2805   8.4989  -0.4604  -0.4608\n",
       "[torch.FloatTensor of size 3x4]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gardf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they are the same as showed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(1))\n",
    "b = Variable(torch.rand(1), requires_grad = True)\n",
    "w = Variable(torch.rand(1), requires_grad = True)\n",
    "y = w * x\n",
    "z = y + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd会跟随用户的操作，记录生成当前variable的所有操作，并构建出一个有向无环图。每次进行一个操作，相应的计算图就会发生改变。链式法则，计算输入的各个Variable的梯度。每一个前向操作的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf, w.is_leaf, b.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf, z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward1 at 0x2ce9515d978>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward1 at 0x2ce9512d710>, 0), (<AccumulateGrad at 0x2ce9512d6a0>, 0))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions[0][0] == y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad_fn, x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward1 at 0x2ce9512d710>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x2ce950b95c0>, 0), (None, 0))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MulBackward1' object has no attribute 'saved_variables'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-177-3ba15276330c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'MulBackward1' object has no attribute 'saved_variables'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "y.grad_fn.saved_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播过程中非叶子结点的导师计算完之后即会被清空。\n",
    "- autograd.grad\n",
    "- hook\n",
    "\n",
    "实际使用的过程要避免修改grad的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(3), requires_grad=True)\n",
    "w = Variable(torch.ones(3), requires_grad=True)\n",
    "y = x * w\n",
    "# y 是依赖于w\n",
    "z = y.sum()\n",
    "x.requires_grad, w.requires_grad, y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " [torch.FloatTensor of size 3], Variable containing:\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " [torch.FloatTensor of size 3], None)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward()\n",
    "(x.grad, w.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第一种方法：使用grad获取中间变量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1\n",
       "  1\n",
       "  1\n",
       " [torch.FloatTensor of size 3],)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(3), requires_grad=True)\n",
    "w = Variable(torch.ones(3), requires_grad=True)\n",
    "y = x * w\n",
    "# y 是依赖于w\n",
    "z = y.sum()\n",
    "# z对y的梯度，隐式调用backward()\n",
    "torch.autograd.grad(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 第二种方法：使用hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y的梯度：\n",
      " Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hook是一个函数，输入为梯度\n",
    "def variable_hook(grad):\n",
    "    print('y的梯度：\\r\\n', grad)\n",
    "\n",
    "x = Variable(torch.ones(3), requires_grad=True)\n",
    "w = Variable(torch.ones(3), requires_grad=True)\n",
    "y = x * w\n",
    "# y 是依赖于w\n",
    "z = y.sum()\n",
    "# 注册hook\n",
    "hook_handle = y.register_hook(variable_hook)\n",
    "# BP\n",
    "z.backward()\n",
    "# 用完之后记得移除hook\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2\n",
       " 4\n",
       " 6\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.arange(3), requires_grad=True)\n",
    "y = x ** 2 + x * 2\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2\n",
       " 4\n",
       " 6\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.arange(3), requires_grad=True)\n",
    "y = x ** 2 + x * 2\n",
    "z = y.sum()\n",
    "y_grad_variables = Variable(torch.Tensor([1,1,1])) # dz/dy\n",
    "y.backward(y_grad_variables) # 从y开始进行反向传播\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有对variable的操作才能够使用autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 扩展autograd\n",
    "\n",
    "写一个function进行扩展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class MultiplyAdd(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b):\n",
    "        print('type in forward ', type(x))\n",
    "        ctx.save_for_backward(w, x)\n",
    "        output = w * x + b\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        w, x = ctx.saved_variables\n",
    "        print('type in backward ', type(x))\n",
    "        grad_w = grad_output * x\n",
    "        grad_x = grad_output * w\n",
    "        grad_b = grad_output * 1\n",
    "        return grad_w, grad_x, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type in backward start forward propagation <class 'torch.autograd.variable.Variable'>\n",
      "\n",
      "type in forward  <class 'torch.FloatTensor'>\n",
      "start back propagation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(1))\n",
    "w = Variable(torch.rand(1), requires_grad=True)\n",
    "b = Variable(torch.rand(1), requires_grad=True)\n",
    "print('start forward propagation')\n",
    "z = MultiplyAdd.apply(w, x, b)\n",
    "print('start back propagation')\n",
    "z.backward()\n",
    "\n",
    "# show grad\n",
    "x.grad, w.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start forward propagation\n",
      "type in forward  <class 'torch.FloatTensor'>\n",
      "start back propagation\n",
      "type in backward  <class 'torch.autograd.variable.Variable'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  0.3917\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(1))\n",
    "w = Variable(torch.rand(1), requires_grad=True)\n",
    "b = Variable(torch.rand(1), requires_grad=True)\n",
    "print('start forward propagation')\n",
    "z = MultiplyAdd.apply(w, x, b)\n",
    "print('start back propagation')\n",
    "\n",
    "# 调用MultiplyAdd.backward\n",
    "# 输出grad_w, grad_x, grad_b\n",
    "z.grad_fn.apply(Variable(torch.ones(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forwad函数的输入为tensor，而backward函数的输入则是variable，为了实现高阶求导。backward函数的输入和返回值都是Variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  10\n",
       " [torch.FloatTensor of size 1],)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([5]), requires_grad=True)\n",
    "y = x ** 2\n",
    "grad_x = torch.autograd.grad(y, x, create_graph=True)\n",
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_grad_x = torch.autograd.grad(grad_x[0], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  2\n",
       " [torch.FloatTensor of size 1],)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用这种方法实现Sigmoid的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = 1 / (1 + torch.exp(-x))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output,  = ctx.saved_variables\n",
    "        grad_x = output + (1 - output) * grad_output\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "for output no. 0,\n numerical:(\n\nColumns 0 to 9 \n 0.2497  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.2490  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.2315  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0868  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.2270  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.1063  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2413  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0699  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2203  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1996\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 10 to 11 \n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.2377  0.0000\n 0.0000  0.1113\n[torch.FloatTensor of size 12x12]\n,)\nanalytical:(\n\nColumns 0 to 9 \n 1.0000  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167\n 0.5311  1.0000  0.5311  0.5311  0.5311  0.5311  0.5311  0.5311  0.5311  0.5311\n 0.3640  0.3640  1.0000  0.3640  0.3640  0.3640  0.3640  0.3640  0.3640  0.3640\n 0.9041  0.9041  0.9041  1.0000  0.9041  0.9041  0.9041  0.9041  0.9041  0.9041\n 0.6515  0.6515  0.6515  0.6515  1.0000  0.6515  0.6515  0.6515  0.6515  0.6515\n 0.8791  0.8791  0.8791  0.8791  0.8791  1.0000  0.8791  0.8791  0.8791  0.8791\n 0.5933  0.5933  0.5933  0.5933  0.5933  0.5933  1.0000  0.5933  0.5933  0.5933\n 0.0756  0.0756  0.0756  0.0756  0.0756  0.0756  0.0756  1.0000  0.0756  0.0756\n 0.6724  0.6724  0.6724  0.6724  0.6724  0.6724  0.6724  0.6724  1.0000  0.6724\n 0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  1.0000\n 0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889\n 0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725\n\nColumns 10 to 11 \n 0.5167  0.5167\n 0.5311  0.5311\n 0.3640  0.3640\n 0.9041  0.9041\n 0.6515  0.6515\n 0.8791  0.8791\n 0.5933  0.5933\n 0.0756  0.0756\n 0.6724  0.6724\n 0.7245  0.7245\n 1.0000  0.3889\n 0.8725  1.0000\n[torch.FloatTensor of size 12x12]\n,)\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-200-7a7e18dc3cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 使用数值逼近的方法不断检验计算梯度的公式是否正确\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[1;34m(func, inputs, eps, atol, rtol, raise_exception)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalytical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0matol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'for output no. %d,\\n numerical:%s\\nanalytical:%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalytical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreentrant\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\gradcheck.py\u001b[0m in \u001b[0;36mfail_test\u001b[1;34m(msg)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: for output no. 0,\n numerical:(\n\nColumns 0 to 9 \n 0.2497  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.2490  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.2315  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0868  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.2270  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.1063  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2413  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0699  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2203  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1996\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 10 to 11 \n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.0000  0.0000\n 0.2377  0.0000\n 0.0000  0.1113\n[torch.FloatTensor of size 12x12]\n,)\nanalytical:(\n\nColumns 0 to 9 \n 1.0000  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167  0.5167\n 0.5311  1.0000  0.5311  0.5311  0.5311  0.5311  0.5311  0.5311  0.5311  0.5311\n 0.3640  0.3640  1.0000  0.3640  0.3640  0.3640  0.3640  0.3640  0.3640  0.3640\n 0.9041  0.9041  0.9041  1.0000  0.9041  0.9041  0.9041  0.9041  0.9041  0.9041\n 0.6515  0.6515  0.6515  0.6515  1.0000  0.6515  0.6515  0.6515  0.6515  0.6515\n 0.8791  0.8791  0.8791  0.8791  0.8791  1.0000  0.8791  0.8791  0.8791  0.8791\n 0.5933  0.5933  0.5933  0.5933  0.5933  0.5933  1.0000  0.5933  0.5933  0.5933\n 0.0756  0.0756  0.0756  0.0756  0.0756  0.0756  0.0756  1.0000  0.0756  0.0756\n 0.6724  0.6724  0.6724  0.6724  0.6724  0.6724  0.6724  0.6724  1.0000  0.6724\n 0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  0.7245  1.0000\n 0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889  0.3889\n 0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725  0.8725\n\nColumns 10 to 11 \n 0.5167  0.5167\n 0.5311  0.5311\n 0.3640  0.3640\n 0.9041  0.9041\n 0.6515  0.6515\n 0.8791  0.8791\n 0.5933  0.5933\n 0.0756  0.0756\n 0.6724  0.6724\n 0.7245  0.7245\n 1.0000  0.3889\n 0.8725  1.0000\n[torch.FloatTensor of size 12x12]\n,)\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "test_input = Variable(torch.randn(3,4), requires_grad=True)\n",
    "# 使用数值逼近的方法不断检验计算梯度的公式是否正确\n",
    "torch.autograd.gradcheck(Sigmoid.apply, (test_input, ), eps=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ce916a2588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1922378540039062 3.038869857788086\n",
      "2.0902440547943115 3.0357255935668945\n",
      "1.976881980895996 3.0239150524139404\n",
      "2.1568245887756348 3.0396687984466553\n",
      "2.0844297409057617 3.032634973526001\n",
      "2.020876407623291 3.0284459590911865\n",
      "1.923807978630066 3.019551992416382\n",
      "1.8112807273864746 3.019566535949707\n",
      "1.9423569440841675 3.032498836517334\n",
      "1.7863578796386719 3.013979434967041\n",
      "2.0793795585632324 3.0367186069488525\n",
      "1.9585503339767456 3.0299715995788574\n",
      "2.044748306274414 3.0349032878875732\n",
      "2.0430705547332764 3.0358269214630127\n",
      "2.006028890609741 3.0384089946746826\n",
      "1.9898625612258911 3.0331430435180664\n",
      "1.9268356561660767 3.029975652694702\n",
      "1.8990051746368408 3.0261588096618652\n",
      "2.0410661697387695 3.0348074436187744\n",
      "2.0642499923706055 3.0397510528564453\n",
      "2.207226514816284 3.051987886428833\n",
      "1.969315767288208 3.035388469696045\n",
      "1.8311249017715454 3.028118133544922\n",
      "2.0640289783477783 3.054611921310425\n",
      "1.8569252490997314 3.038882255554199\n",
      "1.8341275453567505 3.0361711978912354\n",
      "2.1048712730407715 3.0580477714538574\n",
      "1.9480061531066895 3.0429787635803223\n",
      "1.8819050788879395 3.0400757789611816\n",
      "2.1544907093048096 3.0627188682556152\n",
      "1.8408799171447754 3.0426700115203857\n",
      "2.0901050567626953 3.0549075603485107\n",
      "2.0566470623016357 3.0521628856658936\n",
      "2.121067762374878 3.0555102825164795\n",
      "2.016735553741455 3.06003475189209\n",
      "1.8893163204193115 3.0556068420410156\n",
      "1.869920015335083 3.0554983615875244\n",
      "1.904443621635437 3.0592901706695557\n",
      "2.0267183780670166 3.0662217140197754\n",
      "2.082440137863159 3.0736584663391113\n",
      "2.0602917671203613 3.0648810863494873\n",
      "1.986424446105957 3.0614748001098633\n",
      "1.9469343423843384 3.0538320541381836\n",
      "1.8989492654800415 3.0477068424224854\n",
      "1.9940288066864014 3.0592029094696045\n",
      "2.0520846843719482 3.0587494373321533\n",
      "1.9495937824249268 3.048593521118164\n",
      "1.8786191940307617 3.0456626415252686\n",
      "1.8987236022949219 3.047743797302246\n",
      "2.100114583969116 3.0642921924591064\n",
      "2.050572156906128 3.063645362854004\n",
      "2.008463144302368 3.060006856918335\n",
      "2.109281301498413 3.069554090499878\n",
      "1.9536453485488892 3.056335210800171\n",
      "2.1070499420166016 3.0767385959625244\n",
      "1.9623619318008423 3.0675177574157715\n",
      "1.9804102182388306 3.070309638977051\n",
      "1.9010238647460938 3.0618534088134766\n",
      "1.9453924894332886 3.0692555904388428\n",
      "2.0985162258148193 3.073035955429077\n",
      "1.9081449508666992 3.0612940788269043\n",
      "2.023106098175049 3.0671942234039307\n",
      "1.9284863471984863 3.0575084686279297\n",
      "2.0533828735351562 3.0691444873809814\n",
      "1.8014634847640991 3.0516347885131836\n",
      "2.0738189220428467 3.0714499950408936\n",
      "1.9282618761062622 3.0588979721069336\n",
      "2.0913424491882324 3.0674803256988525\n",
      "2.0099523067474365 3.0645596981048584\n",
      "1.8968000411987305 3.0513863563537598\n",
      "2.0122697353363037 3.051664352416992\n",
      "1.9546812772750854 3.053365707397461\n",
      "1.8113154172897339 3.0367050170898438\n",
      "1.805730938911438 3.022108793258667\n",
      "1.9731922149658203 3.0391976833343506\n",
      "1.990372896194458 3.0409533977508545\n",
      "1.956783413887024 3.0437374114990234\n",
      "1.9177378416061401 3.0351266860961914\n",
      "1.8548636436462402 3.0269346237182617\n",
      "1.980576515197754 3.0436511039733887\n",
      "1.9219577312469482 3.0367612838745117\n",
      "1.9698851108551025 3.047544479370117\n",
      "1.9429006576538086 3.0484602451324463\n",
      "1.858776569366455 3.038485288619995\n",
      "1.9893757104873657 3.05110239982605\n",
      "2.01816987991333 3.057508945465088\n",
      "2.065157413482666 3.05944561958313\n",
      "2.0260283946990967 3.051133871078491\n",
      "2.0821025371551514 3.051957130432129\n",
      "1.858963966369629 3.030068874359131\n",
      "2.05025315284729 3.03826904296875\n",
      "2.0481762886047363 3.0297751426696777\n",
      "2.0894079208374023 3.0345263481140137\n",
      "2.005875825881958 3.0275344848632812\n",
      "1.9375910758972168 3.0191750526428223\n",
      "1.9446725845336914 3.02247953414917\n",
      "2.0840468406677246 3.029329538345337\n",
      "2.1075022220611572 3.0331475734710693\n",
      "2.0640830993652344 3.0268349647521973\n",
      "1.7961994409561157 3.0082297325134277\n",
      "1.9656902551651 3.0243897438049316\n",
      "1.9152878522872925 3.021212339401245\n",
      "2.202653408050537 3.0375308990478516\n",
      "1.9974490404129028 3.028756618499756\n",
      "2.0206172466278076 3.026263952255249\n",
      "2.1400461196899414 3.0359575748443604\n",
      "2.1009397506713867 3.0387423038482666\n",
      "2.1061394214630127 3.0426721572875977\n",
      "1.8794865608215332 3.0226755142211914\n",
      "2.1843955516815186 3.0484180450439453\n",
      "1.8645905256271362 3.0259296894073486\n",
      "1.9265435934066772 3.0361828804016113\n",
      "2.0883255004882812 3.0436975955963135\n",
      "1.9567346572875977 3.0310254096984863\n",
      "1.9529383182525635 3.041764497756958\n",
      "1.9842170476913452 3.044180393218994\n",
      "2.007796049118042 3.0523860454559326\n",
      "2.031838893890381 3.056386947631836\n",
      "2.102344274520874 3.0666935443878174\n",
      "2.012618064880371 3.0567007064819336\n",
      "1.9520517587661743 3.057697296142578\n",
      "2.0846660137176514 3.061230182647705\n",
      "1.9960767030715942 3.057328701019287\n",
      "1.9833487272262573 3.056419610977173\n",
      "2.0208210945129395 3.0677080154418945\n",
      "1.9657247066497803 3.063796043395996\n",
      "1.7317016124725342 3.0474212169647217\n",
      "2.2211520671844482 3.0780091285705566\n",
      "1.69571852684021 3.042340040206909\n",
      "1.9670238494873047 3.0644099712371826\n",
      "2.010550022125244 3.072100877761841\n",
      "2.1368818283081055 3.080554485321045\n",
      "1.8432425260543823 3.0657994747161865\n",
      "1.9781603813171387 3.0827345848083496\n",
      "1.9428790807724 3.083486557006836\n",
      "1.9671664237976074 3.0805869102478027\n",
      "1.9904332160949707 3.079101085662842\n",
      "2.0092825889587402 3.0819995403289795\n",
      "2.018906593322754 3.0761489868164062\n",
      "1.9825477600097656 3.0753748416900635\n",
      "1.8916867971420288 3.0687499046325684\n",
      "2.0578439235687256 3.0830681324005127\n",
      "1.8660911321640015 3.0672061443328857\n",
      "1.787190556526184 3.0603132247924805\n",
      "2.0959324836730957 3.08659029006958\n",
      "2.029768943786621 3.0902984142303467\n",
      "1.9941908121109009 3.077594518661499\n",
      "2.1810367107391357 3.089794635772705\n",
      "1.8895978927612305 3.0702173709869385\n",
      "1.993089199066162 3.0835375785827637\n",
      "1.9727275371551514 3.0785605907440186\n",
      "1.9351478815078735 3.0865318775177\n",
      "1.9379713535308838 3.0961267948150635\n",
      "2.00996994972229 3.1068456172943115\n",
      "2.0434651374816895 3.1120352745056152\n",
      "1.8560377359390259 3.0980629920959473\n",
      "1.9658105373382568 3.1018900871276855\n",
      "1.9933972358703613 3.114750623703003\n",
      "2.043735980987549 3.1175825595855713\n",
      "2.1277883052825928 3.1207082271575928\n",
      "1.8752611875534058 3.099797487258911\n",
      "2.1890602111816406 3.127112865447998\n",
      "2.1754322052001953 3.118933916091919\n",
      "1.970027208328247 3.1036183834075928\n",
      "2.0620791912078857 3.1066246032714844\n",
      "2.030884027481079 3.099717140197754\n",
      "1.9090228080749512 3.0893173217773438\n",
      "1.9412544965744019 3.0930819511413574\n",
      "2.029066801071167 3.1023659706115723\n",
      "1.8745291233062744 3.0942327976226807\n",
      "1.9812254905700684 3.1019089221954346\n",
      "1.8890243768692017 3.0985987186431885\n",
      "1.9352591037750244 3.104243278503418\n",
      "1.9614183902740479 3.105525493621826\n",
      "2.1805264949798584 3.121480703353882\n",
      "1.9652345180511475 3.0962400436401367\n",
      "1.9974441528320312 3.0980777740478516\n",
      "1.9055873155593872 3.088289260864258\n",
      "2.0650129318237305 3.097606897354126\n",
      "2.0926315784454346 3.1055283546447754\n",
      "1.9201204776763916 3.0987260341644287\n",
      "1.9291328191757202 3.1024935245513916\n",
      "1.8949790000915527 3.1044116020202637\n",
      "2.108738422393799 3.12239670753479\n",
      "1.852462887763977 3.105238437652588\n",
      "1.999338150024414 3.1114659309387207\n",
      "1.9116963148117065 3.1061365604400635\n",
      "1.9872668981552124 3.105179786682129\n",
      "2.027482271194458 3.1060149669647217\n",
      "1.8221344947814941 3.0900943279266357\n",
      "1.9355744123458862 3.092900037765503\n",
      "1.9681166410446167 3.0937159061431885\n",
      "2.0875322818756104 3.1043801307678223\n",
      "1.9225112199783325 3.0894243717193604\n",
      "1.9747600555419922 3.0982353687286377\n",
      "2.0372138023376465 3.097573757171631\n",
      "1.953244924545288 3.091949224472046\n",
      "2.0666911602020264 3.096080780029297\n",
      "1.862618327140808 3.0825960636138916\n",
      "2.053891897201538 3.092154026031494\n",
      "2.112933874130249 3.087148904800415\n",
      "1.9340845346450806 3.0737757682800293\n",
      "2.1121902465820312 3.0865001678466797\n",
      "1.9959946870803833 3.076603651046753\n",
      "1.9651744365692139 3.073566198348999\n",
      "2.173290729522705 3.092881917953491\n",
      "1.8982892036437988 3.0775442123413086\n",
      "1.9848361015319824 3.090150833129883\n",
      "1.9284369945526123 3.084351062774658\n",
      "1.8686854839324951 3.073195457458496\n",
      "1.9114357233047485 3.086750030517578\n",
      "1.900957703590393 3.081191301345825\n",
      "1.86601722240448 3.0837767124176025\n",
      "2.068204879760742 3.100989818572998\n",
      "1.9500095844268799 3.096072196960449\n",
      "1.9048405885696411 3.088308334350586\n",
      "2.036637544631958 3.0914688110351562\n",
      "1.8876078128814697 3.084774971008301\n",
      "1.9492080211639404 3.0893359184265137\n",
      "2.065340995788574 3.0994579792022705\n",
      "1.84147310256958 3.0837507247924805\n",
      "1.860925555229187 3.084050416946411\n",
      "2.1340746879577637 3.103224754333496\n",
      "2.062300443649292 3.0924906730651855\n",
      "2.085005283355713 3.0970041751861572\n",
      "2.0250134468078613 3.0952634811401367\n",
      "1.7235208749771118 3.0768606662750244\n",
      "2.0996851921081543 3.107647180557251\n",
      "2.0241401195526123 3.1020452976226807\n",
      "2.146509885787964 3.11232590675354\n",
      "2.0439462661743164 3.09964656829834\n",
      "1.9302875995635986 3.092193841934204\n",
      "1.9184644222259521 3.0996668338775635\n",
      "2.1048076152801514 3.113990545272827\n",
      "1.7075895071029663 3.0878727436065674\n",
      "1.7325314283370972 3.088632822036743\n",
      "1.9103158712387085 3.1007795333862305\n",
      "1.8886162042617798 3.0976226329803467\n",
      "2.124596357345581 3.112647533416748\n",
      "1.990349531173706 3.092514753341675\n",
      "1.8943382501602173 3.0857555866241455\n",
      "2.1336452960968018 3.104736328125\n",
      "1.9595226049423218 3.0910632610321045\n",
      "2.0886547565460205 3.0999739170074463\n",
      "2.130037307739258 3.105745553970337\n",
      "1.8690268993377686 3.0892927646636963\n",
      "2.0753841400146484 3.101593017578125\n",
      "1.9286271333694458 3.0916664600372314\n",
      "1.9535932540893555 3.0921337604522705\n",
      "2.0808444023132324 3.1007704734802246\n",
      "2.054828643798828 3.106846332550049\n",
      "1.9698387384414673 3.099954843521118\n",
      "2.078359842300415 3.1071791648864746\n",
      "1.8921282291412354 3.0947039127349854\n",
      "2.019394636154175 3.0999491214752197\n",
      "1.9760088920593262 3.0938103199005127\n",
      "2.108962059020996 3.106323719024658\n",
      "1.912336826324463 3.0897138118743896\n",
      "1.9235000610351562 3.093210220336914\n",
      "1.8971401453018188 3.0979979038238525\n",
      "2.018700361251831 3.1100728511810303\n",
      "2.0559349060058594 3.119589328765869\n",
      "2.002932548522949 3.110827684402466\n",
      "2.0915188789367676 3.1156108379364014\n",
      "2.051096200942993 3.1126270294189453\n",
      "1.9734632968902588 3.1042652130126953\n",
      "1.9901286363601685 3.099766969680786\n",
      "2.1592867374420166 3.117461919784546\n",
      "1.920197606086731 3.0874850749969482\n",
      "1.996141791343689 3.096825122833252\n",
      "1.8496798276901245 3.0911521911621094\n",
      "2.1397225856781006 3.1115479469299316\n",
      "2.0848467350006104 3.1114354133605957\n",
      "2.0827949047088623 3.1151556968688965\n",
      "2.135185956954956 3.1174869537353516\n",
      "1.9760767221450806 3.1080448627471924\n",
      "1.9920045137405396 3.1115846633911133\n",
      "1.9327894449234009 3.1057775020599365\n",
      "1.9607105255126953 3.106564521789551\n",
      "1.9988722801208496 3.108886480331421\n",
      "2.2753751277923584 3.126516342163086\n",
      "2.0895540714263916 3.1112184524536133\n",
      "2.1948611736297607 3.1213748455047607\n",
      "1.7481645345687866 3.0926311016082764\n",
      "2.236873149871826 3.1199283599853516\n",
      "1.8861100673675537 3.0960137844085693\n",
      "1.9550039768218994 3.0942628383636475\n",
      "1.886283040046692 3.08063006401062\n",
      "2.02498459815979 3.0965194702148438\n",
      "2.022228956222534 3.0924434661865234\n",
      "1.9399884939193726 3.088010549545288\n",
      "2.0037331581115723 3.0997257232666016\n",
      "1.8839595317840576 3.091398000717163\n",
      "1.99714195728302 3.1016972064971924\n",
      "2.057265043258667 3.109215497970581\n",
      "1.988172173500061 3.1012461185455322\n",
      "1.9564034938812256 3.09635591506958\n",
      "1.993614912033081 3.0899977684020996\n",
      "1.9691846370697021 3.085393190383911\n",
      "1.8352961540222168 3.0715491771698\n",
      "1.931398630142212 3.0788800716400146\n",
      "2.096071720123291 3.0905840396881104\n",
      "1.8702176809310913 3.0764448642730713\n",
      "2.072772979736328 3.093851327896118\n",
      "2.0618937015533447 3.0991923809051514\n",
      "1.8820096254348755 3.082589864730835\n",
      "2.02833890914917 3.0961921215057373\n",
      "2.1955783367156982 3.106464385986328\n",
      "2.0800132751464844 3.092923402786255\n",
      "2.0120983123779297 3.093125581741333\n",
      "2.0875720977783203 3.1015775203704834\n",
      "1.9865670204162598 3.0907275676727295\n",
      "2.002980947494507 3.0940568447113037\n",
      "1.8927698135375977 3.078845977783203\n",
      "1.909893274307251 3.080918788909912\n",
      "2.0218939781188965 3.0894501209259033\n",
      "2.0965046882629395 3.0915422439575195\n",
      "1.9578825235366821 3.0796916484832764\n",
      "2.045482873916626 3.08241605758667\n",
      "1.7973365783691406 3.0735995769500732\n",
      "2.1234354972839355 3.099688768386841\n",
      "2.0475940704345703 3.0963845252990723\n",
      "2.016605854034424 3.088379144668579\n",
      "2.0876245498657227 3.0933337211608887\n",
      "2.1613473892211914 3.103456497192383\n",
      "1.9760217666625977 3.088054895401001\n",
      "1.8830758333206177 3.0779173374176025\n",
      "2.033937454223633 3.0845046043395996\n",
      "1.819712519645691 3.064253807067871\n",
      "1.8618263006210327 3.0656261444091797\n",
      "2.1335296630859375 3.0815274715423584\n",
      "2.138697862625122 3.0770440101623535\n",
      "1.9577583074569702 3.0671792030334473\n",
      "1.8686864376068115 3.056368827819824\n",
      "1.9050194025039673 3.053250789642334\n",
      "1.9919593334197998 3.0626463890075684\n",
      "2.14218807220459 3.0760433673858643\n",
      "2.060201406478882 3.0664851665496826\n",
      "1.9980131387710571 3.0585238933563232\n",
      "1.853155255317688 3.052337408065796\n",
      "1.8237278461456299 3.055269241333008\n",
      "1.850205659866333 3.065072536468506\n",
      "1.9831979274749756 3.0658605098724365\n",
      "2.1251862049102783 3.0839366912841797\n",
      "1.8816864490509033 3.0636792182922363\n",
      "1.798967957496643 3.054503917694092\n",
      "1.975606083869934 3.070709228515625\n",
      "2.074122428894043 3.072427272796631\n",
      "2.001734733581543 3.0666868686676025\n",
      "1.945528268814087 3.0597639083862305\n",
      "1.9882677793502808 3.0628068447113037\n",
      "1.778964877128601 3.049072742462158\n",
      "1.994219422340393 3.0620298385620117\n",
      "1.995450496673584 3.056025505065918\n",
      "1.9370535612106323 3.0540542602539062\n",
      "1.9712653160095215 3.0536422729492188\n",
      "1.96475350856781 3.0534706115722656\n",
      "1.9080159664154053 3.051748275756836\n",
      "2.0064404010772705 3.058450937271118\n",
      "2.018289089202881 3.052110195159912\n",
      "1.982588768005371 3.0512583255767822\n",
      "2.0733582973480225 3.0556459426879883\n",
      "1.8901798725128174 3.044281005859375\n",
      "1.917130470275879 3.048978567123413\n",
      "1.923985242843628 3.0493392944335938\n",
      "1.993557095527649 3.0541794300079346\n",
      "1.860798716545105 3.051663637161255\n",
      "1.9453413486480713 3.058065176010132\n",
      "1.9198538064956665 3.05340838432312\n",
      "1.9985917806625366 3.055225372314453\n",
      "1.7969971895217896 3.0440969467163086\n",
      "1.9437800645828247 3.0534582138061523\n",
      "1.9490612745285034 3.046220064163208\n",
      "1.9819152355194092 3.050224781036377\n",
      "2.0345005989074707 3.0629680156707764\n",
      "2.0066187381744385 3.0668787956237793\n",
      "2.1145706176757812 3.0782430171966553\n",
      "2.018433094024658 3.0716748237609863\n",
      "2.036182165145874 3.0728847980499268\n",
      "2.030292510986328 3.077683925628662\n",
      "1.9632726907730103 3.0728046894073486\n",
      "1.9040002822875977 3.064277410507202\n",
      "2.078223466873169 3.0756654739379883\n",
      "1.9041292667388916 3.0660347938537598\n",
      "1.9779579639434814 3.0753800868988037\n",
      "2.081418752670288 3.079667568206787\n",
      "1.8807854652404785 3.065546989440918\n",
      "2.0054357051849365 3.078049898147583\n",
      "1.9875895977020264 3.0743019580841064\n",
      "2.0915515422821045 3.076911687850952\n",
      "1.9885667562484741 3.0747547149658203\n",
      "1.799770712852478 3.0589609146118164\n",
      "1.8210049867630005 3.057750940322876\n",
      "1.9167506694793701 3.066155195236206\n",
      "2.1507999897003174 3.0835049152374268\n",
      "2.0790836811065674 3.0802342891693115\n",
      "1.9433188438415527 3.0695950984954834\n",
      "2.04524827003479 3.079721212387085\n",
      "1.8574105501174927 3.056922197341919\n",
      "1.8290517330169678 3.0478122234344482\n",
      "1.999316930770874 3.0603134632110596\n",
      "1.8781039714813232 3.0459866523742676\n",
      "1.8767077922821045 3.04832124710083\n",
      "2.1086137294769287 3.0702569484710693\n",
      "1.9591646194458008 3.0594141483306885\n",
      "1.8338100910186768 3.0497236251831055\n",
      "1.9818708896636963 3.067875623703003\n",
      "2.0162007808685303 3.067147731781006\n",
      "1.9552273750305176 3.062742233276367\n",
      "2.0337893962860107 3.067685127258301\n",
      "2.1386795043945312 3.0714328289031982\n",
      "1.9140615463256836 3.055492639541626\n",
      "2.0495190620422363 3.0728366374969482\n",
      "1.7563005685806274 3.0553507804870605\n",
      "2.098048686981201 3.076489210128784\n",
      "1.9915814399719238 3.0675156116485596\n",
      "1.9964673519134521 3.0666370391845703\n",
      "2.0391359329223633 3.0732765197753906\n",
      "1.972442388534546 3.072927236557007\n",
      "2.141698122024536 3.0868148803710938\n",
      "2.0112125873565674 3.0918262004852295\n",
      "2.1148290634155273 3.09397292137146\n",
      "2.083375930786133 3.0923900604248047\n",
      "1.9955267906188965 3.085646629333496\n",
      "2.1612484455108643 3.102975606918335\n",
      "2.108419179916382 3.0996432304382324\n",
      "1.9552414417266846 3.0808122158050537\n",
      "1.9619337320327759 3.083885669708252\n",
      "2.190220355987549 3.103508234024048\n",
      "1.9014257192611694 3.0919220447540283\n",
      "2.0707008838653564 3.1025218963623047\n",
      "2.076011896133423 3.1007773876190186\n",
      "2.0560646057128906 3.1045303344726562\n",
      "1.7892184257507324 3.0841004848480225\n",
      "2.0543365478515625 3.1013431549072266\n",
      "1.7203199863433838 3.0752546787261963\n",
      "2.2437171936035156 3.113332748413086\n",
      "1.7912451028823853 3.085587501525879\n",
      "2.133355140686035 3.1029751300811768\n",
      "1.8952410221099854 3.0891501903533936\n",
      "1.9166570901870728 3.090853691101074\n",
      "2.049999713897705 3.1000850200653076\n",
      "2.011206865310669 3.0926342010498047\n",
      "2.097364902496338 3.102652072906494\n",
      "1.973832130432129 3.0861141681671143\n",
      "1.9312727451324463 3.0843987464904785\n",
      "1.9146336317062378 3.0794854164123535\n",
      "1.916877269744873 3.0784032344818115\n",
      "1.9437845945358276 3.0799505710601807\n",
      "2.004225969314575 3.0867865085601807\n",
      "2.0299596786499023 3.087228775024414\n",
      "2.064410924911499 3.0907793045043945\n",
      "2.1545753479003906 3.098433017730713\n",
      "1.9472920894622803 3.0803098678588867\n",
      "2.0674400329589844 3.0963661670684814\n",
      "2.0144259929656982 3.093421220779419\n",
      "1.844375491142273 3.0820610523223877\n",
      "1.889649510383606 3.0755257606506348\n",
      "2.021369457244873 3.0800697803497314\n",
      "1.860119104385376 3.069732666015625\n",
      "1.929316520690918 3.0764734745025635\n",
      "1.8807107210159302 3.0778353214263916\n",
      "1.912018060684204 3.078524351119995\n",
      "1.8745713233947754 3.0760419368743896\n",
      "2.0113131999969482 3.0871622562408447\n",
      "1.8117245435714722 3.0737907886505127\n",
      "2.122509002685547 3.095024585723877\n",
      "1.8544249534606934 3.0812957286834717\n",
      "2.079775810241699 3.1032822132110596\n",
      "1.905181646347046 3.097161054611206\n",
      "2.0749967098236084 3.1123807430267334\n",
      "1.7135992050170898 3.079632520675659\n",
      "2.092639923095703 3.106776237487793\n",
      "1.8947782516479492 3.0959279537200928\n",
      "2.0291285514831543 3.1100573539733887\n",
      "1.9479167461395264 3.1024739742279053\n",
      "2.233633518218994 3.1274421215057373\n",
      "2.045811176300049 3.1145706176757812\n",
      "2.0130906105041504 3.105863094329834\n",
      "2.1217596530914307 3.119969606399536\n",
      "1.9024839401245117 3.100959062576294\n",
      "2.190080165863037 3.116492986679077\n",
      "1.8973830938339233 3.0865542888641357\n",
      "2.055234432220459 3.099665880203247\n",
      "2.1476833820343018 3.1031086444854736\n",
      "1.9268728494644165 3.0899605751037598\n",
      "2.049250364303589 3.1038825511932373\n",
      "1.8402072191238403 3.088397264480591\n",
      "1.9266507625579834 3.0972156524658203\n",
      "2.0298619270324707 3.105750799179077\n",
      "2.0961310863494873 3.1063568592071533\n",
      "1.9860432147979736 3.094168186187744\n",
      "1.9597597122192383 3.0929806232452393\n",
      "1.9536213874816895 3.0881054401397705\n",
      "2.0704784393310547 3.093184232711792\n",
      "2.070512056350708 3.0976827144622803\n",
      "1.9481909275054932 3.087911367416382\n",
      "2.3176279067993164 3.106031894683838\n",
      "2.0988876819610596 3.0898523330688477\n",
      "1.9969984292984009 3.085629940032959\n",
      "2.0289101600646973 3.093529224395752\n",
      "1.7666919231414795 3.069777488708496\n",
      "1.8813207149505615 3.076242685317993\n",
      "1.936531662940979 3.078902244567871\n",
      "1.9646258354187012 3.0851545333862305\n",
      "1.930259108543396 3.0884861946105957\n",
      "1.9231433868408203 3.0917584896087646\n",
      "1.8434345722198486 3.0816569328308105\n",
      "1.9682977199554443 3.090256452560425\n",
      "1.8679723739624023 3.081831693649292\n",
      "2.067424774169922 3.093787670135498\n",
      "1.9481608867645264 3.0789968967437744\n",
      "1.9063020944595337 3.071012020111084\n",
      "2.2447011470794678 3.0931789875030518\n",
      "2.0799734592437744 3.0788943767547607\n",
      "1.8933194875717163 3.0701661109924316\n",
      "1.9763637781143188 3.081009864807129\n",
      "2.0168967247009277 3.078195810317993\n",
      "2.082326889038086 3.087466239929199\n",
      "2.00217342376709 3.080878973007202\n",
      "1.9006199836730957 3.0726771354675293\n",
      "1.9193434715270996 3.0803210735321045\n",
      "1.8558907508850098 3.0724194049835205\n",
      "1.975732445716858 3.0878190994262695\n",
      "1.8542840480804443 3.080322265625\n",
      "2.2714426517486572 3.103114604949951\n",
      "1.9135618209838867 3.0783379077911377\n",
      "2.0576539039611816 3.091207981109619\n",
      "1.8297580480575562 3.0639255046844482\n",
      "1.9052307605743408 3.069082021713257\n",
      "1.8801417350769043 3.0699899196624756\n",
      "2.082613468170166 3.0895705223083496\n",
      "1.9689536094665527 3.0839920043945312\n",
      "1.8314977884292603 3.0739498138427734\n",
      "1.9399503469467163 3.0807859897613525\n",
      "2.016990900039673 3.079148292541504\n",
      "1.8959850072860718 3.074885368347168\n",
      "2.1853713989257812 3.0962934494018555\n",
      "2.1377458572387695 3.091907024383545\n",
      "1.9493677616119385 3.0795044898986816\n",
      "2.0526676177978516 3.0868449211120605\n",
      "1.7987091541290283 3.067518711090088\n",
      "2.133004665374756 3.0856144428253174\n",
      "2.017728567123413 3.0671310424804688\n",
      "1.7681461572647095 3.0500028133392334\n",
      "2.1070501804351807 3.0678482055664062\n",
      "1.9606622457504272 3.0542287826538086\n",
      "1.9847139120101929 3.0542540550231934\n",
      "2.020831823348999 3.0635228157043457\n",
      "1.8976032733917236 3.0536980628967285\n",
      "1.897480845451355 3.053633451461792\n",
      "1.9707173109054565 3.054241180419922\n",
      "1.8319438695907593 3.0425381660461426\n",
      "1.9424738883972168 3.0541322231292725\n",
      "1.910300374031067 3.057170867919922\n",
      "1.9563031196594238 3.0584681034088135\n",
      "2.1316475868225098 3.0701305866241455\n",
      "2.0172438621520996 3.061431884765625\n",
      "2.0362014770507812 3.0679433345794678\n",
      "1.8266692161560059 3.0525598526000977\n",
      "1.9215137958526611 3.06139874458313\n",
      "1.887937068939209 3.058673620223999\n",
      "1.7887319326400757 3.0515503883361816\n",
      "2.1121156215667725 3.0755667686462402\n",
      "2.100457191467285 3.0740554332733154\n",
      "2.217043399810791 3.0807793140411377\n",
      "2.0212037563323975 3.0670363903045654\n",
      "2.0802669525146484 3.0754857063293457\n",
      "1.9681525230407715 3.0588557720184326\n",
      "2.0595550537109375 3.068479299545288\n",
      "2.048558473587036 3.066659450531006\n",
      "2.0050976276397705 3.0612220764160156\n",
      "2.001016616821289 3.0586283206939697\n",
      "2.166949987411499 3.071284294128418\n",
      "1.7899062633514404 3.04211688041687\n",
      "1.9968599081039429 3.0593411922454834\n",
      "2.1375749111175537 3.0690574645996094\n",
      "1.8791992664337158 3.049685478210449\n",
      "2.1253418922424316 3.073176145553589\n",
      "2.040215015411377 3.069467544555664\n",
      "2.036282777786255 3.0654077529907227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.045233726501465 3.060148000717163\n",
      "1.9657299518585205 3.051987409591675\n",
      "1.842242956161499 3.0356626510620117\n",
      "1.9179736375808716 3.042064905166626\n",
      "2.0578644275665283 3.0455636978149414\n",
      "1.8964191675186157 3.0396130084991455\n",
      "2.029017686843872 3.0446434020996094\n",
      "1.9821676015853882 3.0406110286712646\n",
      "2.041050910949707 3.046067476272583\n",
      "1.9804697036743164 3.0364902019500732\n",
      "2.0454182624816895 3.0417089462280273\n",
      "2.0890517234802246 3.0418694019317627\n",
      "1.9894418716430664 3.032301187515259\n",
      "2.023502826690674 3.035142183303833\n",
      "1.9335368871688843 3.023137092590332\n",
      "1.8853733539581299 3.0247607231140137\n",
      "1.9090825319290161 3.0291738510131836\n",
      "2.1355016231536865 3.050959587097168\n",
      "2.0413129329681396 3.0407114028930664\n",
      "1.9411404132843018 3.0301096439361572\n",
      "2.1174657344818115 3.043384075164795\n",
      "1.6954694986343384 3.0100297927856445\n",
      "1.9551562070846558 3.03129506111145\n",
      "2.0509119033813477 3.038646936416626\n",
      "2.121356725692749 3.041811466217041\n",
      "2.0857157707214355 3.039701461791992\n",
      "2.0299017429351807 3.0337367057800293\n",
      "1.9967715740203857 3.0289576053619385\n",
      "2.04390549659729 3.0321907997131348\n",
      "1.799621820449829 3.0236549377441406\n",
      "1.9631744623184204 3.0371720790863037\n",
      "2.102307081222534 3.0545332431793213\n",
      "2.0062966346740723 3.0530266761779785\n",
      "1.9703062772750854 3.0549163818359375\n",
      "1.9022839069366455 3.045761823654175\n",
      "1.8928608894348145 3.0602877140045166\n",
      "2.0304980278015137 3.0687191486358643\n",
      "2.0359435081481934 3.0703506469726562\n",
      "1.9120358228683472 3.0596611499786377\n",
      "2.0240747928619385 3.0691139698028564\n",
      "1.980946660041809 3.066089153289795\n",
      "1.9663364887237549 3.0641064643859863\n",
      "2.0410215854644775 3.0665414333343506\n",
      "2.004457712173462 3.063039779663086\n",
      "1.8420679569244385 3.053550958633423\n",
      "2.0585103034973145 3.073211669921875\n",
      "2.0921289920806885 3.0779802799224854\n",
      "2.01080322265625 3.074577569961548\n",
      "2.1349194049835205 3.0778589248657227\n",
      "2.0840163230895996 3.076653242111206\n",
      "1.8996552228927612 3.075240135192871\n",
      "2.20562744140625 3.0944783687591553\n",
      "1.871882677078247 3.066324472427368\n",
      "1.9507542848587036 3.0743489265441895\n",
      "2.2074921131134033 3.0941250324249268\n",
      "2.0005931854248047 3.0827581882476807\n",
      "1.8863226175308228 3.073045492172241\n",
      "1.8661566972732544 3.070053815841675\n",
      "1.9269447326660156 3.0706443786621094\n",
      "1.9040371179580688 3.065444231033325\n",
      "1.9408066272735596 3.068819522857666\n",
      "1.907478928565979 3.0715231895446777\n",
      "2.0457491874694824 3.0763375759124756\n",
      "2.1884536743164062 3.0767295360565186\n",
      "2.0500781536102295 3.0681560039520264\n",
      "1.839771032333374 3.0587737560272217\n",
      "2.1176347732543945 3.0793282985687256\n",
      "1.8928340673446655 3.0640008449554443\n",
      "2.107372999191284 3.082594633102417\n",
      "2.0421245098114014 3.085500955581665\n",
      "2.007280111312866 3.0705912113189697\n",
      "2.0037524700164795 3.0746800899505615\n",
      "1.8989992141723633 3.0675766468048096\n",
      "1.9105395078659058 3.0703811645507812\n",
      "1.8794515132904053 3.0708518028259277\n",
      "2.045818567276001 3.0814831256866455\n",
      "2.0458977222442627 3.0744216442108154\n",
      "1.9699909687042236 3.0720326900482178\n",
      "2.1459715366363525 3.078847646713257\n",
      "2.0892090797424316 3.0682756900787354\n",
      "1.9531694650650024 3.058382272720337\n",
      "1.8830310106277466 3.0515830516815186\n",
      "2.033092498779297 3.0554986000061035\n",
      "2.0289716720581055 3.0487751960754395\n",
      "2.001248598098755 3.0356860160827637\n",
      "1.8701481819152832 3.022280693054199\n",
      "1.951720952987671 3.0243988037109375\n",
      "1.9187595844268799 3.0219860076904297\n",
      "2.017982006072998 3.0297954082489014\n",
      "2.0056471824645996 3.019824981689453\n",
      "2.052454948425293 3.0198252201080322\n",
      "1.800079107284546 2.9973087310791016\n",
      "1.817472219467163 2.9930648803710938\n",
      "1.9626750946044922 2.9985673427581787\n",
      "2.2093822956085205 3.0197908878326416\n",
      "2.120237350463867 3.0087711811065674\n",
      "2.1108975410461426 3.0145957469940186\n",
      "1.9999381303787231 3.000434160232544\n",
      "2.120121479034424 3.012287139892578\n",
      "2.2160890102386475 3.0160071849823\n",
      "2.0308287143707275 3.001039981842041\n",
      "1.9578384160995483 2.9976606369018555\n",
      "2.0085620880126953 3.001845598220825\n",
      "1.9356324672698975 2.990250825881958\n",
      "2.0545847415924072 3.0034267902374268\n",
      "1.8517446517944336 2.9904816150665283\n",
      "2.0337040424346924 3.003861427307129\n",
      "1.9343034029006958 3.0031638145446777\n",
      "1.7674167156219482 2.986196994781494\n",
      "2.175868511199951 3.013181209564209\n",
      "2.083256483078003 3.0083670616149902\n",
      "1.9856078624725342 3.0074894428253174\n",
      "1.9356811046600342 2.999706506729126\n",
      "1.8014445304870605 2.9894003868103027\n",
      "1.850386142730713 2.989736318588257\n",
      "2.250147819519043 3.0193097591400146\n",
      "2.0473642349243164 3.0035011768341064\n",
      "1.9643054008483887 3.004178285598755\n",
      "2.2875609397888184 3.0327932834625244\n",
      "1.8223040103912354 3.0019497871398926\n",
      "2.184225082397461 3.026202440261841\n",
      "2.1762545108795166 3.0207433700561523\n",
      "2.0693860054016113 3.018850564956665\n",
      "1.9757053852081299 2.997426986694336\n",
      "1.817891240119934 2.9851505756378174\n",
      "2.1361725330352783 3.0071675777435303\n",
      "1.962449550628662 2.990898847579956\n",
      "1.8267831802368164 2.9826340675354004\n",
      "1.9783620834350586 2.9860329627990723\n",
      "2.137601852416992 2.9992165565490723\n",
      "1.8181301355361938 2.9756155014038086\n",
      "1.946359634399414 2.987678289413452\n",
      "1.947070837020874 2.991185426712036\n",
      "2.088649272918701 2.995075225830078\n",
      "1.8463860750198364 2.9742534160614014\n",
      "1.9303537607192993 2.975710868835449\n",
      "2.0964415073394775 2.986107587814331\n",
      "1.9436715841293335 2.967712879180908\n",
      "2.022519826889038 2.9718053340911865\n",
      "2.00178861618042 2.9649431705474854\n",
      "2.1231436729431152 2.971381425857544\n",
      "2.0448873043060303 2.9604227542877197\n",
      "2.1310620307922363 2.962488889694214\n",
      "1.8770318031311035 2.9456379413604736\n",
      "1.8295954465866089 2.9465818405151367\n",
      "1.779434323310852 2.9343435764312744\n",
      "2.0007553100585938 2.9568769931793213\n",
      "1.8792996406555176 2.9469141960144043\n",
      "1.9347878694534302 2.9572739601135254\n",
      "2.03830885887146 2.954711675643921\n",
      "1.9441334009170532 2.9500389099121094\n",
      "2.0799171924591064 2.9635016918182373\n",
      "2.0633692741394043 2.9615304470062256\n",
      "2.0601251125335693 2.958855628967285\n",
      "2.06402587890625 2.957953691482544\n",
      "2.1534721851348877 2.9626898765563965\n",
      "1.9345943927764893 2.9483485221862793\n",
      "2.106687068939209 2.960050582885742\n",
      "1.853271245956421 2.933645725250244\n",
      "2.124075412750244 2.9562737941741943\n",
      "1.985176682472229 2.9351742267608643\n",
      "1.9207974672317505 2.9316940307617188\n",
      "2.1296818256378174 2.95011830329895\n",
      "1.9474070072174072 2.9415862560272217\n",
      "1.9657219648361206 2.943249464035034\n",
      "2.1941611766815186 2.9621529579162598\n",
      "2.0715229511260986 2.9465384483337402\n",
      "1.9963747262954712 2.9423623085021973\n",
      "2.0238380432128906 2.9427719116210938\n",
      "1.9070924520492554 2.936260461807251\n",
      "2.0509033203125 2.9548532962799072\n",
      "2.0378708839416504 2.9514315128326416\n",
      "2.060859203338623 2.9554877281188965\n",
      "2.1260602474212646 2.9579086303710938\n",
      "2.0945732593536377 2.9612133502960205\n",
      "2.007700204849243 2.9597270488739014\n",
      "2.067983627319336 2.960545539855957\n",
      "1.8080631494522095 2.9396419525146484\n",
      "2.5306999683380127 2.979893922805786\n",
      "1.7815666198730469 2.930332899093628\n",
      "2.12532639503479 2.963930130004883\n",
      "2.0427684783935547 2.96028733253479\n",
      "1.9903450012207031 2.959371566772461\n",
      "1.9789972305297852 2.955479145050049\n",
      "2.0228867530822754 2.9620325565338135\n",
      "1.8403722047805786 2.946448564529419\n",
      "1.9713325500488281 2.953878164291382\n",
      "2.0386602878570557 2.9539425373077393\n",
      "1.8537381887435913 2.9392287731170654\n",
      "1.9233490228652954 2.9399492740631104\n",
      "2.017843008041382 2.95483660697937\n",
      "2.1422128677368164 2.9647414684295654\n",
      "1.9958974123001099 2.9545910358428955\n",
      "1.9189990758895874 2.947482109069824\n",
      "1.9490981101989746 2.9464426040649414\n",
      "2.04496431350708 2.957235097885132\n",
      "2.2930312156677246 2.972203254699707\n",
      "1.9707006216049194 2.942349910736084\n",
      "1.8450796604156494 2.936018466949463\n",
      "1.801674246788025 2.941497564315796\n",
      "2.0419514179229736 2.9581851959228516\n",
      "1.783154010772705 2.9433987140655518\n",
      "1.963564157485962 2.96366548538208\n",
      "2.0645275115966797 2.969606876373291\n",
      "1.9713809490203857 2.961142063140869\n",
      "2.016761541366577 2.967395782470703\n",
      "2.0525829792022705 2.9768271446228027\n",
      "1.9125362634658813 2.9543397426605225\n",
      "2.030278444290161 2.958056688308716\n",
      "2.0598573684692383 2.963893175125122\n",
      "1.9331820011138916 2.9473180770874023\n",
      "1.6992878913879395 2.923981189727783\n",
      "1.7974544763565063 2.9275832176208496\n",
      "2.1150929927825928 2.9582319259643555\n",
      "1.9775848388671875 2.9476258754730225\n",
      "2.0114736557006836 2.9577107429504395\n",
      "2.1037378311157227 2.965271234512329\n",
      "1.9670379161834717 2.9542787075042725\n",
      "2.0242137908935547 2.953036308288574\n",
      "2.040445327758789 2.956958770751953\n",
      "1.9568675756454468 2.952193260192871\n",
      "1.9870015382766724 2.9535412788391113\n",
      "1.9973411560058594 2.959348678588867\n",
      "2.084491729736328 2.966160535812378\n",
      "1.9348723888397217 2.951575517654419\n",
      "2.1216530799865723 2.9646754264831543\n",
      "1.937272548675537 2.9550790786743164\n",
      "2.253187894821167 2.979642868041992\n",
      "2.0367848873138428 2.9630825519561768\n",
      "1.937282919883728 2.9564239978790283\n",
      "1.8206506967544556 2.9432127475738525\n",
      "2.184898853302002 2.958824634552002\n",
      "1.7774549722671509 2.9371063709259033\n",
      "2.0392041206359863 2.959165096282959\n",
      "2.0820202827453613 2.962510824203491\n",
      "1.940460205078125 2.9521470069885254\n",
      "1.936792254447937 2.954271078109741\n",
      "2.081058979034424 2.961127996444702\n",
      "1.9333138465881348 2.940312147140503\n",
      "2.0398972034454346 2.945923328399658\n",
      "2.046177387237549 2.9450016021728516\n",
      "2.045510768890381 2.946510076522827\n",
      "2.0475833415985107 2.940851926803589\n",
      "2.092358112335205 2.943087100982666\n",
      "2.060537338256836 2.946857452392578\n",
      "2.1007707118988037 2.9464216232299805\n",
      "1.8410338163375854 2.9212441444396973\n",
      "2.050504446029663 2.9358785152435303\n",
      "2.1205198764801025 2.9475831985473633\n",
      "1.8808327913284302 2.9301326274871826\n",
      "2.0279200077056885 2.937751054763794\n",
      "2.1328680515289307 2.941904306411743\n",
      "1.7345439195632935 2.915987730026245\n",
      "2.071511745452881 2.942267417907715\n",
      "2.1444485187530518 2.94866943359375\n",
      "1.8441475629806519 2.930659532546997\n",
      "2.184126377105713 2.957272529602051\n",
      "1.8531080484390259 2.937166452407837\n",
      "1.8479722738265991 2.939986228942871\n",
      "2.059739589691162 2.964451551437378\n",
      "2.029939889907837 2.960325002670288\n",
      "1.971181869506836 2.953800678253174\n",
      "1.922367811203003 2.952564001083374\n",
      "2.0151681900024414 2.957547664642334\n",
      "1.7761954069137573 2.9426493644714355\n",
      "1.8665897846221924 2.9524407386779785\n",
      "2.1916136741638184 2.9723572731018066\n",
      "2.3403425216674805 2.9828481674194336\n",
      "2.0526444911956787 2.953951597213745\n",
      "1.997398018836975 2.9527485370635986\n",
      "2.096513271331787 2.9534239768981934\n",
      "2.192899465560913 2.960310220718384\n",
      "1.9392170906066895 2.9460465908050537\n",
      "2.0654680728912354 2.9670848846435547\n",
      "2.007070779800415 2.9516613483428955\n",
      "2.0509891510009766 2.9581174850463867\n",
      "2.2276220321655273 2.97361159324646\n",
      "2.1017839908599854 2.962139129638672\n",
      "1.9938846826553345 2.953324794769287\n",
      "1.8570048809051514 2.9430079460144043\n",
      "1.8986821174621582 2.9394519329071045\n",
      "2.1416282653808594 2.9601547718048096\n",
      "2.0855069160461426 2.9574451446533203\n",
      "1.88875412940979 2.943643808364868\n",
      "2.1109468936920166 2.9573535919189453\n",
      "1.8004931211471558 2.9354264736175537\n",
      "2.081847667694092 2.9598889350891113\n",
      "2.0639429092407227 2.9609591960906982\n",
      "2.0597214698791504 2.965033769607544\n",
      "2.1784560680389404 2.9732327461242676\n",
      "1.8943125009536743 2.958584785461426\n",
      "2.1836907863616943 2.97796893119812\n",
      "1.8683593273162842 2.9627153873443604\n",
      "2.1562070846557617 2.9760663509368896\n",
      "1.7816667556762695 2.953390121459961\n",
      "2.1204442977905273 2.9775257110595703\n",
      "1.9383755922317505 2.9653565883636475\n",
      "2.060426950454712 2.977431535720825\n",
      "1.946542501449585 2.9665920734405518\n",
      "2.084500789642334 2.980116844177246\n",
      "2.1634044647216797 2.9899463653564453\n",
      "2.011202096939087 2.9802534580230713\n",
      "1.9241372346878052 2.977949380874634\n",
      "1.9483399391174316 2.9834136962890625\n",
      "2.013538360595703 2.980189561843872\n",
      "2.147186040878296 2.9902114868164062\n",
      "2.0845062732696533 2.991999864578247\n",
      "1.9703221321105957 2.9848432540893555\n",
      "2.0583107471466064 2.9994685649871826\n",
      "2.03743314743042 2.9947900772094727\n",
      "1.8739652633666992 2.9885504245758057\n",
      "1.9925341606140137 3.003296375274658\n",
      "1.9544768333435059 3.005934000015259\n",
      "2.0103092193603516 3.007524013519287\n",
      "1.8826916217803955 2.9949607849121094\n",
      "2.1002354621887207 3.008594512939453\n",
      "1.9203150272369385 2.9958298206329346\n",
      "1.9558219909667969 3.007277011871338\n",
      "2.1401524543762207 3.025299549102783\n",
      "1.8963351249694824 3.0028183460235596\n",
      "1.9641456604003906 3.004155397415161\n",
      "1.7978640794754028 2.997197151184082\n",
      "2.1479427814483643 3.027707099914551\n",
      "2.031559944152832 3.0212743282318115\n",
      "2.0517640113830566 3.021476984024048\n",
      "1.988992691040039 3.0184707641601562\n",
      "2.1486668586730957 3.033987522125244\n",
      "2.056298017501831 3.03350830078125\n",
      "2.0541839599609375 3.0301313400268555\n",
      "1.8808670043945312 3.0206854343414307\n",
      "2.0727133750915527 3.037975788116455\n",
      "1.820175051689148 3.0236783027648926\n",
      "1.9591269493103027 3.0324742794036865\n",
      "1.9857414960861206 3.0309877395629883\n",
      "1.953883409500122 3.0267539024353027\n",
      "1.9760079383850098 3.028477907180786\n",
      "1.9636754989624023 3.0297508239746094\n",
      "1.9977556467056274 3.028700828552246\n",
      "2.1887264251708984 3.044560432434082\n",
      "1.945428490638733 3.024418354034424\n",
      "2.1079628467559814 3.045025587081909\n",
      "1.7902642488479614 3.027540445327759\n",
      "2.0603280067443848 3.04650616645813\n",
      "1.935761570930481 3.033522129058838\n",
      "1.9644397497177124 3.0401358604431152\n",
      "1.9110000133514404 3.0382239818573\n",
      "2.155825614929199 3.0585997104644775\n",
      "1.9934712648391724 3.0378825664520264\n",
      "2.1057910919189453 3.042619228363037\n",
      "2.1693801879882812 3.047168493270874\n",
      "2.097740888595581 3.040475606918335\n",
      "1.9778639078140259 3.037588119506836\n",
      "1.9463870525360107 3.0374791622161865\n",
      "2.089756965637207 3.0519213676452637\n",
      "1.9937512874603271 3.0430901050567627\n",
      "2.0656375885009766 3.0399277210235596\n",
      "2.047172784805298 3.032330274581909\n",
      "1.847462773323059 3.0122361183166504\n",
      "2.0269014835357666 3.0241854190826416\n",
      "2.003761053085327 3.0215799808502197\n",
      "2.1105082035064697 3.0302484035491943\n",
      "1.8041495084762573 3.0021772384643555\n",
      "1.9611926078796387 3.0135233402252197\n",
      "1.872676134109497 3.01373553276062\n",
      "2.0351502895355225 3.0194339752197266\n",
      "2.03202748298645 3.020110845565796\n",
      "1.9040241241455078 3.0141947269439697\n",
      "2.2793779373168945 3.0408194065093994\n",
      "2.143442153930664 3.026397705078125\n",
      "2.133774757385254 3.028883934020996\n",
      "1.9756500720977783 3.0137457847595215\n",
      "2.1107113361358643 3.029411792755127\n",
      "1.8758997917175293 3.002934217453003\n",
      "2.0392556190490723 3.0184242725372314\n",
      "2.0440592765808105 3.0175395011901855\n",
      "2.066246747970581 3.022595167160034\n",
      "1.9459822177886963 3.0126171112060547\n",
      "1.983857274055481 3.015634775161743\n",
      "1.8869470357894897 3.011512041091919\n",
      "2.2011797428131104 3.0388333797454834\n",
      "2.0515730381011963 3.021517753601074\n",
      "2.0143232345581055 3.017878532409668\n",
      "1.9489295482635498 3.0126609802246094\n",
      "2.080909013748169 3.023254871368408\n",
      "2.022509813308716 3.018476963043213\n",
      "2.0822348594665527 3.023979663848877\n",
      "2.066894292831421 3.023200273513794\n",
      "1.8419699668884277 2.9937760829925537\n",
      "1.9585353136062622 3.0003843307495117\n",
      "1.8986036777496338 2.99043345451355\n",
      "1.9710043668746948 3.00002121925354\n",
      "2.217477560043335 3.021277904510498\n",
      "2.1028335094451904 3.0133683681488037\n",
      "2.116288185119629 3.013430118560791\n",
      "1.9499592781066895 2.996523380279541\n",
      "2.052220106124878 3.0085630416870117\n",
      "2.0301594734191895 3.006622552871704\n",
      "1.997322916984558 2.999300003051758\n",
      "2.0019733905792236 3.004087448120117\n",
      "2.058180570602417 3.0100293159484863\n",
      "1.8544872999191284 2.995192050933838\n",
      "1.9469547271728516 3.0015509128570557\n",
      "2.017883062362671 3.0144898891448975\n",
      "1.9195749759674072 3.0078723430633545\n",
      "1.9183733463287354 3.0067849159240723\n",
      "2.130680561065674 3.0217063426971436\n",
      "2.025092601776123 3.012838125228882\n",
      "2.1698551177978516 3.0232667922973633\n",
      "1.7866615056991577 2.99381160736084\n",
      "2.1023902893066406 3.012274980545044\n",
      "2.1124825477600098 3.0129637718200684\n",
      "1.9275468587875366 2.998385190963745\n",
      "2.0297248363494873 3.00642728805542\n",
      "1.9409202337265015 2.9969239234924316\n",
      "2.0157480239868164 2.997586727142334\n",
      "1.9569363594055176 2.994072675704956\n",
      "1.9022480249404907 2.992720127105713\n",
      "2.169567108154297 3.009685516357422\n",
      "2.0340054035186768 3.0035009384155273\n"
     ]
    }
   ],
   "source": [
    "# use Variable linear regression\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "torch.manual_seed(666)\n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' random data y = x * 2 + 3 '''\n",
    "    x = torch.rand(batch_size, 1)*20\n",
    "    y = x * 2 + (1 + torch.randn(batch_size, 1))*3\n",
    "    return x, y\n",
    "\n",
    "x, y = get_fake_data()\n",
    "plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# init parameters\n",
    "w = Variable(torch.rand(1, 1), requires_grad=True)\n",
    "b = Variable(torch.zeros(1,1), requires_grad=True)\n",
    "lr = 0.001\n",
    "\n",
    "for ii in range(8000):\n",
    "    x, y = get_fake_data()\n",
    "    x, y = Variable(x), Variable(y)\n",
    "\n",
    "    # forward to calculate the loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y)\n",
    "    loss = 0.5 * (y_pred - y) ** 2\n",
    "    loss = loss.sum()\n",
    "\n",
    "    # back\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameter\n",
    "    w.data.sub_(lr * w.grad.data)\n",
    "    b.data.sub_(lr * b.grad.data)\n",
    "\n",
    "    # zero grad\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    if ii % 1000 == 0:\n",
    "        # draw\n",
    "        display.clear_output(wait=True)\n",
    "        x = torch.arange(0, 20).view(-1, 1)\n",
    "        y = x.mm(w.data) + b.data.expand_as(x)\n",
    "        plt.plot(x.numpy(), y.numpy()) # predicted\n",
    "\n",
    "        x2, y2 = get_fake_data(batch_size=20)\n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "\n",
    "        plt.xlim(0, 20)\n",
    "        plt.ylim(0, 41)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "\n",
    "    print(w.data.squeeze()[0], b.data.squeeze()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "autograd不需要手动计算梯度，在每次反向传播之前需要先将梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
